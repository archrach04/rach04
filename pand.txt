import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, mean_squared_error, r2_score,
    confusion_matrix, ConfusionMatrixDisplay
)
import math

# 1. Create Sample Data
data = {
    'Age': [25, 30, 45, 35, 50, np.nan, 40, 60, 55, np.nan],
    'Income': [50000, 60000, 80000, 70000, 90000, 75000, np.nan, 100000, 95000, 85000],
    'Gender': ['Male', 'Female', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Male', 'Female'],
    'Purchased': [0, 1, 1, 0, 1, 0, 1, 0, 1, 1]
}
df = pd.DataFrame(data)

# 2. Fill Missing Values
df['Age'].fillna(df['Age'].mean(), inplace=True)
df['Income'].fillna(df['Income'].median(), inplace=True)

# 3. Boxplot
plt.figure(figsize=(10, 4))
sns.boxplot(data=df[['Age', 'Income']])
plt.title("Boxplot of Age & Income")
plt.show()

# 4. Histograms
df[['Age', 'Income']].hist(bins=8, figsize=(10, 4), layout=(1, 2), color='skyblue')
plt.suptitle("Histogram of Age and Income")
plt.show()

# 5. Summary Stats
print("\n--- Descriptive Statistics ---")
print(df.describe())

print("\nSkewness:\n", df.skew(numeric_only=True))
print("Kurtosis:\n", df.kurtosis(numeric_only=True))

# 6. Encoding
le = LabelEncoder()
df['Gender'] = le.fit_transform(df['Gender'])  # Male=1, Female=0

# 7. Linear Regression: Income ~ Age
X_lr = df[['Age']]
y_lr = df['Income']

lr = LinearRegression()
lr.fit(X_lr, y_lr)
df['Predicted_Income'] = lr.predict(X_lr)

# Evaluation
r2 = r2_score(y_lr, df['Predicted_Income'])
mse = mean_squared_error(y_lr, df['Predicted_Income'])
rmse = math.sqrt(mse)

print(f"\nLinear Regression — R²: {r2:.3f}, MSE: {mse:.2f}, RMSE: {rmse:.2f}")

# Plot: Linear Regression Line
plt.figure(figsize=(6, 4))
sns.regplot(x='Age', y='Income', data=df, line_kws={"color": "red"})
plt.title("Linear Regression Line")
plt.show()

# Plot: Residual Plot
residuals = y_lr - df['Predicted_Income']
plt.figure(figsize=(6, 4))
plt.scatter(df['Age'], residuals)
plt.axhline(0, color='red', linestyle='--')
plt.title("Residual Plot")
plt.xlabel("Age")
plt.ylabel("Residuals")
plt.show()

# Plot: Regression Error Metrics
plt.figure(figsize=(6, 4))
metrics = ['R2', 'MSE', 'RMSE']
values = [r2, mse, rmse]
sns.barplot(x=metrics, y=values, palette='viridis')
plt.title("Linear Regression Metrics")
plt.show()

# 8. Logistic Regression: Purchased ~ Age + Income + Gender
X_logr = df[['Age', 'Income', 'Gender']]
y_logr = df['Purchased']

X_train, X_test, y_train, y_test = train_test_split(X_logr, y_logr, test_size=0.2, random_state=42)

logr = LogisticRegression()
logr.fit(X_train, y_train)
y_pred = logr.predict(X_test)

acc = accuracy_score(y_test, y_pred)
print(f"\nLogistic Regression Accuracy: {acc:.2f}")

# Confusion Matrix Plot
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logr.classes_)
disp.plot(cmap="Blues")
plt.title("Confusion Matrix - Logistic Regression")
plt.show()

# 9. Correlation Heatmap
plt.figure(figsize=(7, 5))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()


from sklearn.svm import SVC

# Train SVM model
svm = SVC(kernel='linear')  # You can also try 'rbf', 'poly', etc.
svm.fit(X_train, y_train)
svm_pred = svm.predict(X_test)

# Accuracy
svm_acc = accuracy_score(y_test, svm_pred)
print(f"\nSVM Accuracy: {svm_acc:.2f}")

# Confusion Matrix
cm_svm = confusion_matrix(y_test, svm_pred)
disp_svm = ConfusionMatrixDisplay(confusion_matrix=cm_svm, display_labels=svm.classes_)
disp_svm.plot(cmap="Purples")
plt.title("Confusion Matrix - SVM")
plt.show()


from sklearn.naive_bayes import GaussianNB

# Train Naive Bayes model
nb = GaussianNB()
nb.fit(X_train, y_train)
nb_pred = nb.predict(X_test)

# Accuracy
nb_acc = accuracy_score(y_test, nb_pred)
print(f"\nNaive Bayes Accuracy: {nb_acc:.2f}")

# Confusion Matrix
cm_nb = confusion_matrix(y_test, nb_pred)
disp_nb = ConfusionMatrixDisplay(confusion_matrix=cm_nb, display_labels=nb.classes_)
disp_nb.plot(cmap="Oranges")
plt.title("Confusion Matrix - Naive Bayes")
plt.show()


# Accuracy Comparison
plt.figure(figsize=(8, 4))
models = ['Logistic Regression', 'SVM', 'Naive Bayes']
accuracies = [acc, svm_acc, nb_acc]

sns.barplot(x=models, y=accuracies, palette='pastel')
plt.ylim(0, 1)
plt.title("Accuracy Comparison Across Models")
plt.ylabel("Accuracy")
plt.show()
